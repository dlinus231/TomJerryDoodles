{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 11:20:04.270212: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-21 11:20:04.274773: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 11:20:04.375032: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 11:20:04.376100: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 11:20:06.113048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LeakyReLU\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from enum import Enum\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /home/linus/WashUCoding/DIS Copenhagen/Neural Networks/TomJerryProject\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL VARIABLES \n",
    "os.chdir(\"/home/linus/WashUCoding/DIS Copenhagen/Neural Networks/TomJerryProject\")\n",
    "print(\"Working Directory:\", os.getcwd())\n",
    "# directories containing desired output, i.e. our y-labels, the ground truth \n",
    "outputDirectories = [\"archive/tom_and_jerry/tom_and_jerry/jerry\", \n",
    "                     \"archive/tom_and_jerry/tom_and_jerry/tom\", \n",
    "                    #  \"archive/tom_and_jerry/tom_and_jerry/tom_jerry_0\",\n",
    "                     \"archive/tom_and_jerry/tom_and_jerry/tom_jerry_1\"]\n",
    "\n",
    "# directories containing edge-filtered images\n",
    "inputDirectories = [\"archive/tom_and_jerry/tom_and_jerry_edge_detected/jerry_edge_detected\", \n",
    "                    \"archive/tom_and_jerry/tom_and_jerry_edge_detected/tom_edge_detected\", \n",
    "                    # \"archive/tom_and_jerry/tom_and_jerry_edge_detected/tom_jerry_0_edge_detected\",\n",
    "                    \"archive/tom_and_jerry/tom_and_jerry_edge_detected/tom_jerry_1_edge_detected\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\"\"\" Returns file paths for all images in the input and output directories\n",
    "\n",
    "    Returns:\n",
    "        List(Tuples): each tuple contains paths to two corresponding images, the first is the edge-filtered image, the second is the unaltered image\n",
    "\"\"\"\n",
    "def getFilePaths(): \n",
    "    dataset = []\n",
    "    for inputDir, outputDir in zip(inputDirectories, outputDirectories): \n",
    "        fileNames = os.listdir(inputDir)\n",
    "        for fileName in fileNames: \n",
    "            inputImagePath = os.path.join(inputDir, fileName)\n",
    "            outputImagePath = os.path.join(outputDir, fileName)\n",
    "            dataset.append((inputImagePath, outputImagePath))\n",
    "    return dataset\n",
    "\n",
    "# takes in two paths of paired images, \n",
    "# optionally, one can specify the desired resolution of the image\n",
    "# whether to display the retrieved images, and whether to convert the images to RGB\n",
    "# will return the two images as numpy arrays, normalized by dividing by 255\n",
    "\"\"\" Returns two images as two numpy arrays, normalized by dividing by 255\n",
    "\n",
    "    Args: \n",
    "        input_file_path (str): path to an edge-filtered image\n",
    "        output_file_path (str): path corresponding to the un-modified image of the input_file_path\n",
    "        image_shape Tuple(int, int): desired width and length of the image\n",
    "        showImages (boolean): whether to display both images using matplotlib\n",
    "        isRGB (boolean): whether to convert\n",
    "\n",
    "    Returns: \n",
    "        (np.array, np.array): Two rescaled numpy arrays, representing two images\n",
    "\"\"\"\n",
    "def loadImage(input_file_path, output_file_path, \n",
    "              image_shape=(400, 200), showImages=False, isRGB = True): \n",
    "\n",
    "    input_image = Image.open(input_file_path).resize(image_shape)\n",
    "    output_image = Image.open(output_file_path).resize(image_shape)\n",
    "    beforeGrayScale = np.array(input_image)\n",
    "    # print(\"Inside loadImage():\", np.min(beforeGrayScale), np.max(beforeGrayScale))\n",
    "\n",
    "    # convert to greyscale if desired\n",
    "    if not isRGB: \n",
    "        input_image = input_image.convert(\"L\")\n",
    "        output_image = output_image.convert(\"L\")\n",
    "\n",
    "    # if desired, display the retrieved images\n",
    "    if showImages: \n",
    "        # Display images for testing\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(np.array(input_image))\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(np.array(output_image))\n",
    "        plt.title(\"Ground Truth Image\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "    # for some reason, max pixel value from edge_detected images is 118.0\n",
    "    return np.array(input_image)/118.0, np.array(output_image)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = getFilePaths()\n",
    "maxesOfInput = []\n",
    "maxesOfOutput = []\n",
    "# for i in range(len(paths)): \n",
    "#     randomNum = random.randint(0, len(paths)-1)\n",
    "#     result = loadImage(paths[randomNum][0], paths[randomNum][1], image_shape=(28,28), showImages=False, isRGB=False)\n",
    "#     maxesOfInput.append(np.max(result[0]))\n",
    "#     maxesOfOutput.append(np.max(result[1]))\n",
    "    # print(np.min(result[0]), np.max(result[0]))\n",
    "    # print(np.min(result[1]), np.max(result[1]))\n",
    "# print(max(maxesOfInput))\n",
    "# print(max(maxesOfOutput))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION OF TRAINING AND TESTING DATASETS\n",
    "\n",
    "# randomize paths so images are randomly allocated into training/testing datasets\n",
    "# random.shuffle(paths)\n",
    "validation_split = 0.2\n",
    "\n",
    "x_train, y_train = [], []\n",
    "x_test, y_test = [], []\n",
    "\n",
    "imagesProcessed = 0\n",
    "totalImagePairs = len(paths)\n",
    "\n",
    "# THIS VARIABLE AFFECTS THE DIMENSIONS OF ALL IMAGES IN THE DATASET\n",
    "# IT WILL ALSO AFFECT THE SHAPE OF THE AUTO_ENCODER\n",
    "personal_image_shape = (100, 50)\n",
    "for inputImagePath, outputImagePath in paths: \n",
    "    inputImage, imageLabel = loadImage(inputImagePath, outputImagePath, image_shape=personal_image_shape, isRGB=False)\n",
    "\n",
    "    # if validation_split = 0.2, put 80% of images into the training dataset\n",
    "    if imagesProcessed / totalImagePairs > validation_split: \n",
    "        x_train.append(inputImage)\n",
    "        y_train.append(imageLabel)\n",
    "    # else, put the images into the testing dataset\n",
    "    else: \n",
    "        x_test.append(inputImage)\n",
    "        y_test.append(imageLabel)\n",
    "        \n",
    "    imagesProcessed += 1\n",
    "\n",
    "\n",
    "# print(np.min(result[0]/255.0), np.max(result[0]/255.0))\n",
    "# print(np.min(result[1]/255.0), np.max(result[1]/255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3159 3159\n",
      "791 791\n",
      "(3159, 50, 100)\n",
      "(3159, 50, 100)\n",
      "(791, 50, 100)\n",
      "(791, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "# Confirming sizes of datsets and shapes of the datasets\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))\n",
    "x_train, y_train, x_test, y_test = np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following guidance from the below link\n",
    "# https://www.tensorflow.org/tutorials/load_data/numpy\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderWithDense(Model):\n",
    "    def __init__(self, latent_dim, dropout_rate=0.2):\n",
    "        super(AutoencoderWithDense, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1024, activation=\"relu\"), # Added Dense\n",
    "            layers.Dropout(dropout_rate), \n",
    "            layers.Dense(512, activation=\"relu\"), # Added Dense\n",
    "            layers.Dropout(dropout_rate), \n",
    "            layers.Dense(latent_dim, activation='relu'),\n",
    "            ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "\n",
    "            layers.Dense(512, activation=\"relu\"), # Added Dense\n",
    "            layers.Dropout(dropout_rate), \n",
    "            layers.Dense(1024, activation=\"relu\"), # Added Dense\n",
    "            layers.Dropout(dropout_rate), \n",
    "            layers.Dense(personal_image_shape[1] * personal_image_shape[0], activation='sigmoid'),\n",
    "            layers.Reshape((personal_image_shape[1], personal_image_shape[0])) # is reversed from other functions\n",
    "            # layers.Reshape((100, 200)) # is reversed from other functions\n",
    "            ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 53ms/step - loss: 0.0492 - val_loss: 0.0346\n",
      "Epoch 2/1000\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.0311 - val_loss: 0.0326\n",
      "Epoch 3/1000\n",
      "50/50 [==============================] - 3s 53ms/step - loss: 0.0286 - val_loss: 0.0296\n",
      "Epoch 4/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0259 - val_loss: 0.0290\n",
      "Epoch 5/1000\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 0.0243 - val_loss: 0.0272\n",
      "Epoch 6/1000\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.0234 - val_loss: 0.0275\n",
      "Epoch 7/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0227 - val_loss: 0.0287\n",
      "Epoch 8/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0224 - val_loss: 0.0288\n",
      "Epoch 9/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0221 - val_loss: 0.0269\n",
      "Epoch 10/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0218 - val_loss: 0.0259\n",
      "Epoch 11/1000\n",
      "50/50 [==============================] - 3s 53ms/step - loss: 0.0211 - val_loss: 0.0259\n",
      "Epoch 12/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0207 - val_loss: 0.0239\n",
      "Epoch 13/1000\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 0.0200 - val_loss: 0.0236\n",
      "Epoch 14/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0196 - val_loss: 0.0233\n",
      "Epoch 15/1000\n",
      "50/50 [==============================] - 3s 54ms/step - loss: 0.0191 - val_loss: 0.0231\n",
      "Epoch 16/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0184 - val_loss: 0.0236\n",
      "Epoch 17/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0179 - val_loss: 0.0228\n",
      "Epoch 18/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0173 - val_loss: 0.0219\n",
      "Epoch 19/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0169 - val_loss: 0.0216\n",
      "Epoch 20/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0165 - val_loss: 0.0217\n",
      "Epoch 21/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0163 - val_loss: 0.0208\n",
      "Epoch 22/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0157 - val_loss: 0.0211\n",
      "Epoch 23/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0153 - val_loss: 0.0206\n",
      "Epoch 24/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0149 - val_loss: 0.0205\n",
      "Epoch 25/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0146 - val_loss: 0.0208\n",
      "Epoch 26/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0142 - val_loss: 0.0207\n",
      "Epoch 27/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0139 - val_loss: 0.0203\n",
      "Epoch 28/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0134 - val_loss: 0.0206\n",
      "Epoch 29/1000\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.0133 - val_loss: 0.0193\n",
      "Epoch 30/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0128 - val_loss: 0.0193\n",
      "Epoch 31/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0126 - val_loss: 0.0187\n",
      "Epoch 32/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0123 - val_loss: 0.0187\n",
      "Epoch 33/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0121 - val_loss: 0.0188\n",
      "Epoch 34/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0118 - val_loss: 0.0190\n",
      "Epoch 35/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0117 - val_loss: 0.0189\n",
      "Epoch 36/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0114 - val_loss: 0.0193\n",
      "Epoch 37/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0113 - val_loss: 0.0195\n",
      "Epoch 38/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0111 - val_loss: 0.0185\n",
      "Epoch 39/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0109 - val_loss: 0.0185\n",
      "Epoch 40/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0107 - val_loss: 0.0184\n",
      "Epoch 41/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0106 - val_loss: 0.0180\n",
      "Epoch 42/1000\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.0103 - val_loss: 0.0184\n",
      "Epoch 43/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0102 - val_loss: 0.0178\n",
      "Epoch 44/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0102 - val_loss: 0.0176\n",
      "Epoch 45/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0100 - val_loss: 0.0177\n",
      "Epoch 46/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0098 - val_loss: 0.0177\n",
      "Epoch 47/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0097 - val_loss: 0.0175\n",
      "Epoch 48/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0095 - val_loss: 0.0175\n",
      "Epoch 49/1000\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.0094 - val_loss: 0.0171\n",
      "Epoch 50/1000\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.0093 - val_loss: 0.0174\n",
      "Epoch 51/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0092 - val_loss: 0.0169\n",
      "Epoch 52/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0091 - val_loss: 0.0171\n",
      "Epoch 53/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0090 - val_loss: 0.0167\n",
      "Epoch 54/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0089 - val_loss: 0.0169\n",
      "Epoch 55/1000\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.0089 - val_loss: 0.0168\n",
      "Epoch 56/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0088 - val_loss: 0.0172\n",
      "Epoch 57/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0087 - val_loss: 0.0166\n",
      "Epoch 58/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0086 - val_loss: 0.0170\n",
      "Epoch 59/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0086 - val_loss: 0.0163\n",
      "Epoch 60/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0085 - val_loss: 0.0166\n",
      "Epoch 61/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0085 - val_loss: 0.0164\n",
      "Epoch 62/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0085 - val_loss: 0.0161\n",
      "Epoch 63/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0083 - val_loss: 0.0165\n",
      "Epoch 64/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0084 - val_loss: 0.0160\n",
      "Epoch 65/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0081 - val_loss: 0.0160\n",
      "Epoch 66/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0081 - val_loss: 0.0164\n",
      "Epoch 67/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0080 - val_loss: 0.0163\n",
      "Epoch 68/1000\n",
      "50/50 [==============================] - 3s 53ms/step - loss: 0.0080 - val_loss: 0.0161\n",
      "Epoch 69/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0080 - val_loss: 0.0165\n",
      "Epoch 70/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0078 - val_loss: 0.0164\n",
      "Epoch 71/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0078 - val_loss: 0.0163\n",
      "Epoch 72/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0078 - val_loss: 0.0162\n",
      "Epoch 73/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0077 - val_loss: 0.0166\n",
      "Epoch 74/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0077 - val_loss: 0.0160\n",
      "Epoch 75/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0077 - val_loss: 0.0161\n",
      "Epoch 76/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0076 - val_loss: 0.0164\n",
      "Epoch 77/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0075 - val_loss: 0.0164\n",
      "Epoch 78/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0075 - val_loss: 0.0163\n",
      "Epoch 79/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0075 - val_loss: 0.0166\n",
      "Epoch 80/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0074 - val_loss: 0.0162\n",
      "Epoch 81/1000\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.0074 - val_loss: 0.0163\n",
      "Epoch 82/1000\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.0074 - val_loss: 0.0163\n",
      "Epoch 83/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0074 - val_loss: 0.0162\n",
      "Epoch 84/1000\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.0073 - val_loss: 0.0162\n",
      "Epoch 85/1000\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0072 - val_loss: 0.0160\n",
      "Epoch 86/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0072 - val_loss: 0.0161\n",
      "Epoch 87/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0071 - val_loss: 0.0164\n",
      "Epoch 88/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0071 - val_loss: 0.0161\n",
      "Epoch 89/1000\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.0071 - val_loss: 0.0160\n"
     ]
    }
   ],
   "source": [
    "autoencoder_with_dense = AutoencoderWithDense(latent_dim = 250)\n",
    "autoencoder_with_dense.compile(optimizer='adam', \n",
    "                               loss=losses.MeanSquaredError())\n",
    "                              #  loss=losses.MeanAbsoluteError())\n",
    "                              #  loss=losses.BinaryCrossentropy())\n",
    "\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience = 15)\n",
    "\n",
    "history = autoencoder_with_dense.fit(train_dataset,\n",
    "                epochs=1000,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_dataset), \n",
    "                callbacks=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder_with_dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/linus/WashUCoding/DIS Copenhagen/Neural Networks/TomJerryProject/src/DenseModel.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linus/WashUCoding/DIS%20Copenhagen/Neural%20Networks/TomJerryProject/src/DenseModel.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainImageIndex \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(x_train)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linus/WashUCoding/DIS%20Copenhagen/Neural%20Networks/TomJerryProject/src/DenseModel.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m testImageIndex \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(x_test)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/linus/WashUCoding/DIS%20Copenhagen/Neural%20Networks/TomJerryProject/src/DenseModel.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_output_image \u001b[39m=\u001b[39m autoencoder_with_dense\u001b[39m.\u001b[39mpredict(x_train)[trainImageIndex]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linus/WashUCoding/DIS%20Copenhagen/Neural%20Networks/TomJerryProject/src/DenseModel.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_output_image \u001b[39m=\u001b[39m autoencoder_with_dense\u001b[39m.\u001b[39mpredict(x_test)[testImageIndex]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linus/WashUCoding/DIS%20Copenhagen/Neural%20Networks/TomJerryProject/src/DenseModel.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m6\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoder_with_dense' is not defined"
     ]
    }
   ],
   "source": [
    "# Display images for testing\n",
    "\n",
    "trainImageIndex = random.randint(0, len(x_train)-1)\n",
    "testImageIndex = random.randint(0, len(x_test)-1)\n",
    "train_output_image = autoencoder_with_dense.predict(x_train)[trainImageIndex]\n",
    "test_output_image = autoencoder_with_dense.predict(x_test)[testImageIndex]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(np.array(x_train[trainImageIndex]))\n",
    "plt.title(\"Training Input Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(np.array(train_output_image))\n",
    "plt.title(\"Training Output Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(np.array(y_train[trainImageIndex]))\n",
    "plt.title(\"Training Ground Truth Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(np.array(x_test[testImageIndex]))\n",
    "plt.title(\"Testing Input Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(np.array(test_output_image))\n",
    "plt.title(\"Testing Output Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(np.array(y_test[testImageIndex]))\n",
    "plt.title(\"Testing Ground Truth Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_with_dense.save(\"src/SavedModels/DenseModel2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
